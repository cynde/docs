---
title: "Ollama"
description: ""
---

# Ollama

## Integrate Ollama with Jan[](#integrate-ollama-with-jan)

Ollama provides you with large language models that you can run locally. There are two methods to integrate Ollama with Jan:

1.  Integrate the Ollama server with Jan.
2.  Migrate the downloaded model from Ollama to Jan.

To integrate Ollama with Jan, follow the steps below:

This tutorial will show how to integrate Ollama with Jan using the first method. We will use the [llama2 (opens in a new tab)](https://ollama.com/library/llama2) model as an example.

### Step 1: Server Setup[](#step-1-server-setup)

According to the [Ollama documentation on OpenAI compatibility (opens in a new tab)](https://github.com/ollama/ollama/blob/main/docs/openai.md), you can connect to the Ollama server using the web address `http://localhost:11434/v1/chat/completions`. To do this, follow the steps below:

1.  Navigate to the **Settings** > **Model Providers**.
2.  In the **OpenAI** section, add the full web address of the Ollama server.

  

![Server Setup](https://jan.ai/_next/static/media/Ollama-1.2786760a.gif)

Leave the API Key field blank.

### Step 2: Download Model[](#step-2-download-model)

1.  Navigate to the **Hub**.
2.  Download the Ollama model, for example, `Llama 2 Chat 7B Q4`.

  

![Download Model](https://jan.ai/_next/static/media/Ollama-2.d38b0782.gif)

### Step 3: Start the Model[](#step-3-start-the-model)

1.  Navigate to the **Threads**.
2.  Click the **Model** tab.
3.  Select the `Llama 2 Chat 7B Q4` model and configure the model parameters.
4.  Start chatting with the model.

  

![Start Model](https://jan.ai/_next/static/media/Ollama-3.6a955730.gif)

Last updated on July 25, 2024

[LM Studio](/docs/local-models/lmstudio "LM Studio")[OpenAI API](/docs/remote-models/openai "OpenAI API")